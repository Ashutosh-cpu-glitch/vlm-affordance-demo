# vlm-affordance-demo
A minimal implementation exploring Vision-Language Models (VLMs) for object grounding and spatial affordance estimation. This project demonstrates how OpenAI's CLIP can localize objects from text prompts and generate basic graspability heatmaps â€” inspired by recent research in robotic manipulation and embodied AI.
